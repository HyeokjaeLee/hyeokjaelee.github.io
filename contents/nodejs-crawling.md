---
emoji: "📢"
title: "크롤링을 통한 데이터 수집"
description: "Node.js에서 크롤링을 이용해 데이터를 수집해보겠습니다."
date: "2021-08-31"
tag: [Javascript, Data]
---

## 크롤링(Crawling)

<img src="https://user-images.githubusercontent.com/71566740/139003885-ded14d60-8ae6-4308-ba25-7bf6375d7e1a.png" class="img large">

크롤링은 웹 페이지에서 원하는 데이터를 추출해 내는 행위입니다.

크롤링을 위해 개발된 소프트웨어를 크롤러(Crawler)라 합니다.

<br>저는 보통 API를 만들거나 Tensorflow 학습 데이터를 수집하는데 크롤링을 자주 이용합니다.

크롤링에 활용 가능한 도구는 언어별로 Jsoup(Java), BeautifulSoup(Python) 등 여러 종류가 있지만 이번 포스트에서는 제가 가장 자주 사용하는 Javascript, Node.js 도구들을 이용하겠습니다.

(언어마다 도구는 달라도 동작 방식은 대체로 비슷합니다.)

## 도구 선택

node.js에서도 크롤링에 사용할 수 있는 도구도 종류가 많고 그중에 용도에 맞는 도구를 선택하면 되겠습니다.
<br>이 포스트에서는 [OpenInsider](http://openinsider.com/insider-purchases-25k)(해외 내부자 거래 정보 사이트)를 크롤링 해보겠습니다.

<br>해당 페이지는 로그인도 필요 없고 따로 크롤링이 차단되어 있지도 않기 때문에 단순 http 라이브러리와 parsing 라이브러리만 사용하겠습니다.

<br>만약 특정 이유로 사람이 직접 데이터를 수집하는 것처럼 브라우저를 핸들링하는 방법으로 크롤링 해야 한다면 속도는 느리지만 Chromium을 제어하는 도구들(Puppeteer 등)을 사용하시면 됩니다.
<br>(시간이 된다면 Puppeteer 사용방법도 다뤄보도록 하겠습니다.)

### <svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Node.js</title><path d="M11.998,24c-0.321,0-0.641-0.084-0.922-0.247l-2.936-1.737c-0.438-0.245-0.224-0.332-0.08-0.383 c0.585-0.203,0.703-0.25,1.328-0.604c0.065-0.037,0.151-0.023,0.218,0.017l2.256,1.339c0.082,0.045,0.197,0.045,0.272,0l8.795-5.076 c0.082-0.047,0.134-0.141,0.134-0.238V6.921c0-0.099-0.053-0.192-0.137-0.242l-8.791-5.072c-0.081-0.047-0.189-0.047-0.271,0 L3.075,6.68C2.99,6.729,2.936,6.825,2.936,6.921v10.15c0,0.097,0.054,0.189,0.139,0.235l2.409,1.392 c1.307,0.654,2.108-0.116,2.108-0.89V7.787c0-0.142,0.114-0.253,0.256-0.253h1.115c0.139,0,0.255,0.112,0.255,0.253v10.021 c0,1.745-0.95,2.745-2.604,2.745c-0.508,0-0.909,0-2.026-0.551L2.28,18.675c-0.57-0.329-0.922-0.945-0.922-1.604V6.921 c0-0.659,0.353-1.275,0.922-1.603l8.795-5.082c0.557-0.315,1.296-0.315,1.848,0l8.794,5.082c0.57,0.329,0.924,0.944,0.924,1.603 v10.15c0,0.659-0.354,1.273-0.924,1.604l-8.794,5.078C12.643,23.916,12.324,24,11.998,24z M19.099,13.993 c0-1.9-1.284-2.406-3.987-2.763c-2.731-0.361-3.009-0.548-3.009-1.187c0-0.528,0.235-1.233,2.258-1.233 c1.807,0,2.473,0.389,2.747,1.607c0.024,0.115,0.129,0.199,0.247,0.199h1.141c0.071,0,0.138-0.031,0.186-0.081 c0.048-0.054,0.074-0.123,0.067-0.196c-0.177-2.098-1.571-3.076-4.388-3.076c-2.508,0-4.004,1.058-4.004,2.833 c0,1.925,1.488,2.457,3.895,2.695c2.88,0.282,3.103,0.703,3.103,1.269c0,0.983-0.789,1.402-2.642,1.402 c-2.327,0-2.839-0.584-3.011-1.742c-0.02-0.124-0.126-0.215-0.253-0.215h-1.137c-0.141,0-0.254,0.112-0.254,0.253 c0,1.482,0.806,3.248,4.655,3.248C17.501,17.007,19.099,15.91,19.099,13.993z"/></svg> Node.js 라이브러리

#### HTTP 라이브러리: Axios

- http 라이브러리에는 종류가 굉장히 많고 저는 평소 Request를 자주 사용해 왔는데 해당 라이브러리가 deprecated 되었다는 소식을 듣고 이번에는 가장 성능이 좋다는 Axios를 사용해 보기로 했습니다.

#### Parsing 라이브러리: Cheerio

- 사실 parsing 라이브러리는 없어도 직접 파싱 해서 사용할 수 있지만 방대한 량의 html 코드를 파싱 하는 과정이 복잡해질뿐더러 코드의 가독성도 떨어집니다.
  <br>저는 여기서 jQuery 문법을 그대로 사용할 수 있는 Cheerio를 사용해서 파싱 하겠습니다.

## 개발환경 설정

### 새 프로젝트 생성

```bash
$ mkdir <프로젝트 이름>
$ cd <프로젝트 이름>
$ npm init
```

### 사용할 node.js 모듈 설치

```bash
npm install axios cheerio
```

### 크롤링 맛보기

이제 원하는 정보들의 위치를 찾아야 합니다.
<br>원하는 정보를 오른쪽 마우스로 클릭후 검사를 사용하면 쉽게 찾을 수 있습니다.
<br>저는 거래 날짜와 해당 주식의 ticker 값을 받아오기 위한 selector를 복사해보겠습니다.

![capture](https://user-images.githubusercontent.com/71566740/131478329-82d599e6-56fa-44df-b80a-e609896315f8.png)
첫번째 값을 기준으로 각 정보의 slector는 다음과 같은걸 확인 할 수 있습니다.

<br>**거래날짜**: `#tablewrapper > table > tbody > tr:nth-child(1) > td:nth-child(3) > div`
<br>**ticker**: `#tablewrapper > table > tbody > tr:nth-child(1) > td:nth-child(4) > b > a`

<br>이 구조를 보면 한 가지 거래 정보는 같은 tr에 포함되어 있다는 걸 알 수 있습니다.

### 코드 작성

생성한 프로젝트에 index.js 파일을 만들고 코드를 작성합니다.
<br>여러 가지 방법이 있겠지만 저는 map 메서드를 이용해 모든 tr들의 거래 날짜와 정보를 가진 객체 배열을 출력하는 코드로 작성했습니다.

```javascript
const cheerio = require("cheerio");
const axios = require("axios");

(async () => {
  //크롤링 대상 URL, axios의 get은 비동기 함수이므로 async-await을 사용한다.
  const html = await axios.get("http://openinsider.com/insider-purchases-25k"),
    $ = cheerio.load(html.data);

  const trElements = $("#tablewrapper > table > tbody > tr");
  const insiderTradeData = trElements
    .map((index, tr) => ({
      date: $(tr).find("td:nth-child(3) > div").text(),
      ticker: $(tr).find("td:nth-child(4) > b > a").text(),
    }))
    .toArray();
  console.log(insiderTradeData);
})();
```

`node index`로 실행해보면 결과는 다음과 같이 나오는 걸 확인할 수 있습니다.

![131484221-8eaa2b8f-749e-46d9-8efe-486e4630e963](https://user-images.githubusercontent.com/71566740/133531609-93363fba-e51e-47aa-b03a-5cad03bc1795.png)

## 마지막으로

- 위에서 사용했던 http 라이브러리나 parsing 라이브러리 둘 다 크롤링에 활용 가능한 도구이지 '크롤링만을 위한 도구'가 아닙니다.
  <br>해당 라이브러리가 어떤 역할을 하는지 직접 찾아보시는 것도 좋을 것 같습니다.
- 크롤링은 꼭 개발자가 아니더라도 원하는 데이터를 자급자족할 수 있다는 점에서 활용도가 정말 높습니다.
  <br>위에서 다뤘던 예제는 기초적인 크롤링 방법만을 다룬 것이므로 실제 크롤링을 이용해 무언가를 하려면 대상의 URL 규칙성, 페이지의 구조 등을 직접 분석해보면서 코드를 작성하는 방법을 고민해 보셔야 합니다.
- 크롤링은 사람이 직접 데이터를 수집하는 것보다 훨씬 빠른 속도로 서버에 다수에 요청을 보내서 데이터를 응답받기 때문에 크롤링 대상 서버에 문제를 발생시킬 수 있습니다.
  <br> 위와 같은 이유로 크롤링이 차단되어 있는 사이트들도 있으니 크롤링이 허용되어 있는 사이트인지 확인하는 것도 중요합니다.
- 크롤링으로 처벌을 받은 판례가 있으니 실제 서비스를 위한 코드를 작성할 때는 해당 정보가 크롤링이 허용되는 정보인지 잘 확인해 보시고 사용하는 게 좋을 것 같습니다.
